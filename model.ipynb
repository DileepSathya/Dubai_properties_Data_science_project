{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso\n",
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dividing data in training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_csv(\"final_data.csv\")\n",
    "x=df.drop(columns=[\"Rent_per_sqft\"])\n",
    "y=df[\"Rent_per_sqft\"]\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data normaliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scaler=MinMaxScaler(feature_range=(0,1))\\nscaler.fit(x_train)\\nx_train_scaled=scaler.transform(x_train)\\nx_test_scaled=scaler.transform(x_test)'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"scaler=MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled=scaler.transform(x_train)\n",
    "x_test_scaled=scaler.transform(x_test)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=[\"Unnamed: 0\",\"Latitude\",\"Longitude\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(model_name):\n",
    "    model_name.fit(x_train,y_train)\n",
    "\n",
    "    y_train_pred = model_name.predict(x_train)\n",
    "\n",
    "    # Step 3: Calculate R^2 on Training Data\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    print(\"R^2 on \"+str(model_name)+\"Training Data:\", r2_train)\n",
    "\n",
    "    # Step 4: Predictions on Test Data\n",
    "    y_test_pred = model_name.predict(x_test)\n",
    "\n",
    "    # Step 5: Calculate R^2 on Test Data\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    print(\"R^2 on \"+str(model_name)+\"Test Data:\", r2_test)\n",
    "    print((\"=\"*100))\n",
    "\n",
    "    mse_train=mean_squared_error(y_train, y_train_pred)\n",
    "    print(\"mse on \"+str(model_name)+\" Training Data:\", mse_train)\n",
    "\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    print(\"mse on \"+str(model_name)+\"Test Data:\", mse_test)\n",
    "    print((\"=\"*100))\n",
    "    mae_train=mean_absolute_error(y_train, y_train_pred)\n",
    "    print(\"mae on \"+str(model_name)+\" Training Data:\", mae_train)\n",
    "\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    print(\"mae on \"+str(model_name)+\"Test Data:\", mae_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 on GradientBoostingRegressor(learning_rate=0.055, loss='huber', max_depth=11,\n",
      "                          n_estimators=80)Training Data: 0.8337278636601265\n",
      "R^2 on GradientBoostingRegressor(learning_rate=0.055, loss='huber', max_depth=11,\n",
      "                          n_estimators=80)Test Data: 0.8144270508624684\n",
      "====================================================================================================\n",
      "mse on GradientBoostingRegressor(learning_rate=0.055, loss='huber', max_depth=11,\n",
      "                          n_estimators=80) Training Data: 744.2070802301913\n",
      "mse on GradientBoostingRegressor(learning_rate=0.055, loss='huber', max_depth=11,\n",
      "                          n_estimators=80)Test Data: 796.3351625159345\n",
      "====================================================================================================\n",
      "mae on GradientBoostingRegressor(learning_rate=0.055, loss='huber', max_depth=11,\n",
      "                          n_estimators=80) Training Data: 10.39510898682019\n",
      "mae on GradientBoostingRegressor(learning_rate=0.055, loss='huber', max_depth=11,\n",
      "                          n_estimators=80)Test Data: 13.027921617108719\n"
     ]
    }
   ],
   "source": [
    "gbr=GradientBoostingRegressor(n_estimators=80,learning_rate=0.055,max_depth=11,loss=\"huber\",criterion='friedman_mse')\n",
    "model(gbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 on GradientBoostingRegressor(learning_rate=0.055, loss='huber', max_depth=11,\n",
      "                          n_estimators=80)Training Data: 0.8295635968305867\n",
      "R^2 on GradientBoostingRegressor(learning_rate=0.055, loss='huber', max_depth=11,\n",
      "                          n_estimators=80)Test Data: 0.8154142985869614\n",
      "====================================================================================================\n",
      "mse on GradientBoostingRegressor(learning_rate=0.055, loss='huber', max_depth=11,\n",
      "                          n_estimators=80) Training Data: 762.8456622965006\n",
      "mse on GradientBoostingRegressor(learning_rate=0.055, loss='huber', max_depth=11,\n",
      "                          n_estimators=80)Test Data: 792.0986610172976\n",
      "====================================================================================================\n",
      "mae on GradientBoostingRegressor(learning_rate=0.055, loss='huber', max_depth=11,\n",
      "                          n_estimators=80) Training Data: 10.494701537067424\n",
      "mae on GradientBoostingRegressor(learning_rate=0.055, loss='huber', max_depth=11,\n",
      "                          n_estimators=80)Test Data: 13.121884039045472\n"
     ]
    }
   ],
   "source": [
    "gbr=GradientBoostingRegressor(n_estimators=80,learning_rate=0.055,max_depth=11,loss=\"huber\",criterion='friedman_mse')\n",
    "model(gbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
